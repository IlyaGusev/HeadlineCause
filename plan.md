# Plan
- Начать накидывать статью
- Запуск разметки пар на английском (с машинным переводом на русский на всякий случай)
- Уверенная разметка 3к русских и 3к английских пар примеров. В идеале с балансом классов 50/50, можно с перекосом в сторону сложных ноликов. Источники: ссылки Ленты, темы Риа, ссылки Телеграма
- Обучение берта на разметке на 3 класса: следствие слева направо, следствие справа налево, нет следствия (CauseTask). Во-первых, отдельная модель на каждый язык. Во-вторых, одна общая модель. Отдельные модели делаем в двух вариантах: на моноязычных бертах, на мультиязычном берте. Для мультиязычного смотрим качество переноса.
- Отдельную подвыборку выделяем как SuperGLUE-style датасет (CauseTaskHard): не более 500 примеров в обучающей выборке. Приниципы выделения примеров надо обсудить, возможные варианты: наиболее далёкие друг от друга, с наименьшим перекрытием, с наибольшим перекрытием.
- Обучение берта для детекции отмен среди следствий (CancelTask)
- Unsupervised GPT для определения направления следствия (DirectionTask)
- Статистика по используемым парам глаголов.
- Эксперименты с нарративами: пытаемся строить цепочки на основе CauseTask классификатора.
