# Plan

ilya-gusev:
- Бейзлайн (например tf-idf)
- Сделать первую версию чистого датасета
- Начать накидывать статью
- Запуск разметки пар на английском (с машинным переводом на русский на всякий случай)
- Уверенная разметка 3к русских и 3к английских пар примеров. В идеале с балансом классов 50/50, можно с перекосом в сторону сложных ноликов. Источники: ссылки Ленты, темы Риа, ссылки Телеграма.
- Обучение берта на разметке на 3 класса: следствие слева направо, следствие справа налево, нет следствия (CauseTask). Во-первых, отдельная модель на каждый язык. Во-вторых, одна общая модель. Отдельные модели делаем в двух вариантах: на моноязычных бертах, на мультиязычном берте.
- Для мультиязычного смотрим качество переноса.
- Отдельную подвыборку выделяем как SuperGLUE-style датасет (CauseTaskHard): не более 500 примеров в обучающей выборке. Приниципы выделения примеров надо обсудить, возможные варианты: наиболее далёкие друг от друга, с наименьшим перекрытием, с наибольшим перекрытием. 
- Эксперименты с нарративами: пытаемся строить цепочки на основе CauseTask классификатора.

altsoph:
- ~~Обучение берта для детекции отмен среди следствий (CancelTask)~~
- Unsupervised GPT для определения направления следствия (UnsupervisedDirectionTask)
- ~~Найти навык английских толокеров~~ https://toloka.yandex.com/requester/quality/skill/23048 / en_gramm_native
- Статистика по используемым парам глаголов

backlog:
- Правда ли, что там где модель ошибается, согласованность толокеров тоже ниже
