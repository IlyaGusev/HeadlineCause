# Plan

ilya-gusev:
- Бейзлайн (например tf-idf)
- Начать накидывать статью
- ~~Сделать первую версию чистого датасета~~
- ~~Запуск разметки пар на английском (с машинным переводом на русский на всякий случай)~~
- Уверенная разметка минимум 3к русских и 3к английских пар примеров.
- ~~Обучение берта на разметке на 3 класса: следствие слева направо, следствие справа налево, нет следствия (CauseTask)~~. Во-первых, отдельная модель на каждый язык. Во-вторых, одна общая модель. Отдельные модели делаем в двух вариантах: на моноязычных бертах, на мультиязычном берте.
- Для мультиязычного смотрим качество переноса.
- Добавить симметричные примеры в train


altsoph:
- ~~Обучение берта для детекции отмен среди следствий (CancelTask)~~
- ~~Unsupervised GPT для определения направления следствия (UnsupervisedDirectionTask)~~
- ~~Найти навык английских толокеров~~ https://toloka.yandex.com/requester/quality/skill/23048 / en_gramm_native
- ~~Статистика по используемым парам глаголов~~
- ~~Эксперименты с нарративами: пытаемся строить цепочки на основе CauseTask классификатора.~~
- ~~GPT генерация последствий~~

backlog:
- Правда ли, что там где модель ошибается, согласованность толокеров тоже ниже
- Аннотирование мета-данными: оценки bert'а, оценки single-bert'а
- Single-sentence bert для английского
- Dawid-Skene: разобраться в причинах проблемы, построить дифф двух датасетов


deep backlog
- Отдельную подвыборку выделяем как SuperGLUE-style датасет (CauseTaskHard): не более 500 примеров в обучающей выборке. Приниципы выделения примеров надо обсудить, возможные варианты: наиболее далёкие друг от друга, с наименьшим перекрытием, с наибольшим перекрытием. 
